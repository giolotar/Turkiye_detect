{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNEJmIkfme3CNZkOTOx4HCE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giolotar/Turkiye_detect/blob/main/Inference_Runner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Turkey Detect Inference Runner**"
      ],
      "metadata": {
        "id": "mipGA6y2WN_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The following code loads a pretrained model with weights and configuration**\n",
        "\n",
        "The model was trained on the rubble instances of a single sat image of Antakya, Turkyie. Annotations were made in CVAT with one label \"Damage\" = \"Rubble\n",
        "The model weights and configuration are loaded and assembled into a predictor."
      ],
      "metadata": {
        "id": "qZtEQRjFWZGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predictor then ingests a tif image, tiles it, runs predictions on the tiles, based on that predictions it does two things:\n",
        "\n",
        "\n",
        "\n",
        "1.   Generates a masks file .npz with the bounding boxes, coordinates and confidence grade for each prediction. the file which is then used to make a shapefile that is georeferenced to the injected tif\n",
        "2.   Generates a simple png image with the printed masks on it\n",
        "\n"
      ],
      "metadata": {
        "id": "5SqLf-81W0fV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SET-UP**"
      ],
      "metadata": {
        "id": "lfFCxhAFXrZ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "44CaW_qRvXB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b0f6c8-e0ea-41e6-e58e-3922e5795934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mount up the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Detectron2 Library\n",
        "!python -m pip install pyyaml==5.1\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "metadata": {
        "id": "HP6IyM8czlAW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9455832-3adb-411c-9d4b-8d514d68da3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/274.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m266.2/274.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-v709x1ff\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-v709x1ff\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit a1ce2f956a1d2212ad672e3c47d53405c2fe4312\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.0.10)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.19.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-25.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (8.3.0)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (4.4.0)\n",
            "Collecting pytokens>=0.1.10 (from black->detectron2==0.6)\n",
            "  Downloading pytokens-0.1.10-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.75.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.9)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->detectron2==0.6) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.3)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-25.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pytokens-0.1.10-py3-none-any.whl (12 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp312-cp312-linux_x86_64.whl size=6732670 sha256=836490cc521670e97b0ad88957e44d5819fa20571bfffc7069b241a4f59ac9cb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9j_64jcq/wheels/d3/6e/bd/1969578f1456a6be2d6f083da65c669f450b23b8f3d1ac14c1\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=85a43df6b2124dbddb18a9fabdebdf0d8f39bbb623881710c7abe25601ecaa69\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "Successfully built detectron2 fvcore\n",
            "Installing collected packages: yacs, pytokens, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed black-25.9.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 pathspec-0.12.1 portalocker-3.2.0 pytokens-0.1.10 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GPU Check\n",
        "\n",
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ],
      "metadata": {
        "id": "fpp663Az5KAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd89603-79ce-45f5-dc35-229337688505"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "torch:  2.8 ; cuda:  cu126\n",
            "detectron2: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pQ7HdUtkPWaI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# Detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load the Model and Config**\n",
        "\n",
        "Here we load a pretrained model configuration and weights"
      ],
      "metadata": {
        "id": "PPVu1FzuYGTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cfg_pretrained = get_cfg()\n",
        "\n",
        "saved_config_path = \"/content/drive/MyDrive/colabDL/turkey_OGrun.yaml\"\n",
        "saved_weights_path = \"/content/drive/MyDrive/colabDL/tile1/model_final_cfg2.pth\"\n",
        "\n",
        "# Load the configuration directly from your saved .yaml file\n",
        "try:\n",
        "    cfg_pretrained.merge_from_file(saved_config_path)\n",
        "except KeyError as e:\n",
        "    print(f\"Error loading configuration from {saved_config_path}: {e}\")\n",
        "    raise\n",
        "\n",
        "# Set the path to the model weights\n",
        "cfg_pretrained.MODEL.WEIGHTS = saved_weights_path\n",
        "\n",
        "# Ensure the output directory is created (needed for the predictor)\n",
        "cfg_pretrained.OUTPUT_DIR = \"/content/sample_data\"\n",
        "os.makedirs(cfg_pretrained.OUTPUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "SYTihJmqtzhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c50497-f9f3-4baa-8dde-9fa7025f0557"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.config:Loading config /content/drive/MyDrive/colabDL/turkey_OGrun.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Compile Predictor**"
      ],
      "metadata": {
        "id": "fV5KR3FjYvjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Threshold\n",
        "cfg_pretrained.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3\n",
        "\n",
        "# Assemble predictor\n",
        "predictor = DefaultPredictor(cfg_pretrained)\n",
        "\n",
        "print(\"DefaultPredictor initialized successfully with loaded configuration and weights.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QZfxHSgYXJN",
        "outputId": "cca6cbc1-942a-4b86-d681-a23ba014ff25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10/06 09:24:29 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /content/drive/MyDrive/colabDL/tile1/model_final_cfg2.pth ...\n",
            "DefaultPredictor initialized successfully with loaded configuration and weights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to predict on an image / tile see if the predictor works"
      ],
      "metadata": {
        "id": "-bzr4oopfkrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_im = cv2.imread(\"/content/drive/MyDrive/colabDL/tile1/test_row1to6_8/tile_y4095_x7371_row5_col9.tif\")\n",
        "outputs  = predictor(new_im)\n",
        "\n",
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "\n",
        "#!!! NEED TO EXPLORE WITH VISUALIZER OPTIONS MORE APPEALING MASKS REPRESENTATIONS\n",
        "v = Visualizer(new_im[:, :, ::-1], metadata=None)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "krbzoGG0iKLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference**"
      ],
      "metadata": {
        "id": "bex9MRnxWrwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we run the predictor on an unseen Georef image, the image gets tiled, predictions are run on the tiles and exported in .npz as georeferenced polygons.\n",
        "Then they are turned into a shapefile"
      ],
      "metadata": {
        "id": "rsaD3p1ciMkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load georeferenced TIF"
      ],
      "metadata": {
        "id": "EygAnXpPW1e_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65cee5f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf57e70-f733-4419-f067-e60df2cc4eba"
      },
      "source": [
        "!pip install rasterio fiona shapely"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting fiona\n",
            "  Downloading fiona-1.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (2.1.2)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2025.8.3)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.3.0)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.5)\n",
            "Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m121.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fiona-1.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m138.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio, fiona\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 fiona-1.10.1 rasterio-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tile_tiff_save_preds.py\n",
        "import os\n",
        "import math\n",
        "import cv2\n",
        "import numpy as np\n",
        "import rasterio # Import rasterio for georeferenced image handling\n",
        "\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog  # needed for Visualizer\n",
        "\n",
        "# ==== USER PATHS ====\n",
        "INPUT_TIFF = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/1_GEOTIFF.tif\" # Use the georeferenced TIFF\n",
        "OUT_DIR   = \"/content/drive/MyDrive/colabDL/Turkiye/TR1\"  # main output directory\n",
        "NPZ_OUT_DIR = os.path.join(OUT_DIR, \"npz_preds\") # subdirectory for npz files\n",
        "TILE = 1024\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(NPZ_OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Read the georeferenced image and get georeferencing info\n",
        "try:\n",
        "    with rasterio.open(INPUT_TIFF) as src:\n",
        "        img = src.read()\n",
        "        # Rasterio reads bands first (C, H, W), convert to OpenCV format (H, W, C)\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "        # Explicitly select the first 3 channels to handle potential alpha channels\n",
        "        if img.shape[2] == 4:\n",
        "            img = img[:, :, :3]\n",
        "        orig_h, orig_w = img.shape[:2]\n",
        "        transform = src.transform # Affine transform for georeferencing\n",
        "        crs = src.crs # Coordinate reference system\n",
        "\n",
        "except rasterio.errors.RasterioIOError as e:\n",
        "    raise FileNotFoundError(f\"Could not read georeferenced image at {INPUT_TIFF}: {e}\")\n",
        "\n",
        "# Pad to multiples of TILE so we don’t lose right/bottom edges\n",
        "H = math.ceil(orig_h / TILE) * TILE\n",
        "W = math.ceil(orig_w / TILE) * TILE\n",
        "pad_bottom = H - orig_h\n",
        "pad_right  = W - orig_w\n",
        "\n",
        "if pad_bottom or pad_right:\n",
        "    # Use the last pixel value for padding to avoid edge effects if possible,\n",
        "    # otherwise use black padding for color images.\n",
        "    if img.shape[2] == 3:\n",
        "        pad_val = [int(img[-1,-1,0]), int(img[-1,-1,1]), int(img[-1,-1,2])]\n",
        "    else:\n",
        "        pad_val = [0] * img.shape[2]\n",
        "    img = cv2.copyMakeBorder(\n",
        "        img, 0, pad_bottom, 0, pad_right,\n",
        "        borderType=cv2.BORDER_CONSTANT, value=pad_val\n",
        "    )\n",
        "\n",
        "rows = H // TILE\n",
        "cols = W // TILE\n",
        "\n",
        "tiles_written = 0\n",
        "dets_saved = 0\n",
        "\n",
        "for y in range(rows):\n",
        "    for x in range(cols):\n",
        "        r0 = y * TILE\n",
        "        c0 = x * TILE\n",
        "        tile = img[r0:r0+TILE, c0:c0+TILE]\n",
        "\n",
        "        # Calculate georeferenced origin of the tile (top-left corner)\n",
        "        geo_x0, geo_y0 = transform * (c0, r0)\n",
        "\n",
        "        # ---- Run inference on the tile\n",
        "        outputs = predictor(tile)\n",
        "        inst = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "        # ---- Extract raw predictions\n",
        "        boxes  = inst.pred_boxes.tensor.numpy() if inst.has(\"pred_boxes\") else np.zeros((0,4), dtype=np.float32)\n",
        "        scores = inst.scores.numpy()             if inst.has(\"scores\")     else np.zeros((0,), dtype=np.float32)\n",
        "        classes= inst.pred_classes.numpy()       if inst.has(\"pred_classes\") else np.zeros((0,), dtype=np.int32)\n",
        "        masks  = inst.pred_masks.numpy()         if inst.has(\"pred_masks\") else None  # (N, H, W) boolean\n",
        "\n",
        "        # ---- SAVE RAW PREDICTIONS (compressed)\n",
        "        # Include tile origin in pixel and georeferenced coordinates\n",
        "        npz_path = os.path.join(NPZ_OUT_DIR, f\"tile_{y}_{x}_pred.npz\")\n",
        "        np.savez_compressed(\n",
        "            npz_path,\n",
        "            boxes=boxes,\n",
        "            scores=scores,\n",
        "            classes=classes,\n",
        "            masks=masks,                 # None is allowed; np.load(..., allow_pickle=True) will handle it\n",
        "            tile_row=y,\n",
        "            tile_col=x,\n",
        "            tile_origin_rc=np.array([r0, c0], dtype=np.int32), # Pixel origin (row, col)\n",
        "            tile_origin_geo=np.array([geo_x0, geo_y0], dtype=np.float64), # Georeferenced origin (x, y)\n",
        "            tile_size=TILE,\n",
        "            padded_h=H,\n",
        "            padded_w=W,\n",
        "            orig_h=orig_h,\n",
        "            orig_w=orig_w,\n",
        "            pad_bottom=pad_bottom,\n",
        "            pad_right=pad_right\n",
        "        )\n",
        "        dets_saved += len(boxes)\n",
        "\n",
        "print(f\"Done. Wrote {tiles_written} visualized tiles to {OUT_DIR}\")\n",
        "print(f\"Saved predictions for {rows*cols} tiles ({dets_saved} detections total) as .npz files to {NPZ_OUT_DIR}\")"
      ],
      "metadata": {
        "id": "cHUiBpUR5Rr3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4309fc7-00e9-4632-97a5-79c8f000fda9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. Wrote 0 visualized tiles to /content/drive/MyDrive/colabDL/Turkiye/TR1\n",
            "Saved predictions for 80 tiles (157 detections total) as .npz files to /content/drive/MyDrive/colabDL/Turkiye/TR1/npz_preds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert from the npz to shp retaining geographic coordinates"
      ],
      "metadata": {
        "id": "xdXEnh2vUGjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import rasterio\n",
        "from shapely.geometry import Polygon, mapping\n",
        "import fiona"
      ],
      "metadata": {
        "id": "MszJ1WCOUK-J"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing the predicted tile .npz files\n",
        "PREDICTED_NPZ_DIR = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/npz_preds\" # Assuming npz files are in a subdirectory\n",
        "\n",
        "# Output shapefile/geopackage path\n",
        "# Changed to .gpkg for geopackage, which is preferred\n",
        "OUTPUT_VECTOR_PATH = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/Rubble_1.gpkg\"\n",
        "\n",
        "# Path to the original georeferenced image (GeoTIFF or similar)\n",
        "\n",
        "# This is necessary to get the coordinate system and transform information.\n",
        "ORIGINAL_GEOREFERENCED_IMAGE_PATH = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/1_GEOTIFF.tif\" # Example path, replace with your actual georeferenced image path\n",
        "\n",
        "# --- Function to get georeferencing information ---\n",
        "def get_georeferencing_info(image_path):\n",
        "    \"\"\"Reads georeferencing information (transform and CRS) from a georeferenced image.\"\"\"\n",
        "    try:\n",
        "        with rasterio.open(image_path) as src:\n",
        "            return src.transform, src.crs\n",
        "    except rasterio.errors.RasterioIOError as e:\n",
        "        print(f\"Error reading georeferencing info from {image_path}: {e}\")\n",
        "        print(\"Assuming pixel coordinates. The output vector file will NOT be georeferenced.\")\n",
        "        return None, None\n",
        "\n",
        "# Get georeferencing info from the original image\n",
        "transform, crs = get_georeferencing_info(ORIGINAL_GEOREFERENCED_IMAGE_PATH)\n",
        "\n",
        "# --- Define the schema for the vector file ---\n",
        "# This defines the attributes (columns) for each feature (polygon) in the vector file.\n",
        "# We'll include score and class_id from the predictions.\n",
        "schema = {\n",
        "    'geometry': 'Polygon',\n",
        "    'properties': {'score': 'float', 'class_id': 'int'},\n",
        "}\n",
        "\n",
        "# --- Create the vector file (Geopackage) ---\n",
        "# Determine the driver based on the output file extension\n",
        "# Changed driver to GPKG for Geopackage\n",
        "driver = 'GPKG'\n",
        "\n",
        "# Open the vector file in write mode\n",
        "# If a CRS was obtained, use it. Otherwise, the vector file will have no defined CRS.\n",
        "with fiona.open(\n",
        "    OUTPUT_VECTOR_PATH,\n",
        "    'w',\n",
        "    driver,\n",
        "    schema,\n",
        "    crs=crs # Use the obtained CRS, or None if not available\n",
        ") as collection:\n",
        "    # List all .npz files in the directory\n",
        "    npz_filenames = [f for f in os.listdir(PREDICTED_NPZ_DIR) if f.endswith('_pred.npz')]\n",
        "\n",
        "    if not npz_filenames:\n",
        "        print(\"No predicted .npz files found in the directory.\")\n",
        "    else:\n",
        "        # Iterate through each .npz file\n",
        "        for filename in npz_filenames:\n",
        "            npz_path = os.path.join(PREDICTED_NPZ_DIR, filename)\n",
        "            with np.load(npz_path, allow_pickle=True) as data:\n",
        "                masks = data['masks']\n",
        "                scores = data['scores']\n",
        "                classes = data['classes']\n",
        "                tile_origin_rc = data['tile_origin_rc'] # Pixel origin (r0, c0)\n",
        "                # tile_origin_geo = data['tile_origin_geo'] # Georeferenced origin (x0, y0) - not directly used here but available\n",
        "\n",
        "                if masks is not None and masks.shape[0] > 0:\n",
        "                    # Process each instance mask within the tile\n",
        "                    for i in range(masks.shape[0]):\n",
        "                        instance_mask = masks[i]\n",
        "                        score = scores[i]\n",
        "                        class_id = classes[i]\n",
        "\n",
        "                        # Find contours of the mask\n",
        "                        # cv2.findContours requires a single-channel binary image\n",
        "                        # Ensure instance_mask is uint8 and binary (0 or 255)\n",
        "                        mask_uint8 = instance_mask.astype(np.uint8) * 255\n",
        "                        # Use cv2.RETR_TREE and cv2.CHAIN_APPROX_NONE for more detailed contours if needed,\n",
        "                        # but SIMPLE is usually sufficient for polygons.\n",
        "                        contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "                        # Convert contours to polygons and transform coordinates\n",
        "                        for contour in contours:\n",
        "                            # Reshape contour to be a list of points (x, y)\n",
        "                            # Contour points are in tile coordinates (pixel within the tile)\n",
        "                            # Ensure points are in (x, y) format which corresponds to (column, row)\n",
        "                            contour_points_tile = contour.squeeze().tolist()\n",
        "\n",
        "                            if not isinstance(contour_points_tile, list):\n",
        "                                # Handle cases where contour.squeeze() results in a single point\n",
        "                                contour_points_tile = [contour_points_tile]\n",
        "                            elif not all(isinstance(p, list) for p in contour_points_tile):\n",
        "                                # Handle cases where contour.squeeze() results in a flat list of coords\n",
        "                                contour_points_tile = [contour_points_tile]\n",
        "\n",
        "\n",
        "                            if len(contour_points_tile) < 3:\n",
        "                                continue # Skip if not enough points to form a polygon\n",
        "\n",
        "                            # Convert tile pixel coordinates to padded image pixel coordinates\n",
        "                            # Tile origin is (row, col), contour points are (col, row)\n",
        "                            contour_points_padded = [(p[0] + tile_origin_rc[1], p[1] + tile_origin_rc[0]) for p in contour_points_tile]\n",
        "\n",
        "                            # Convert padded image pixel coordinates to geographic coordinates using the original image's transform\n",
        "                            if transform:\n",
        "                                # Apply the affine transform to each point\n",
        "                                # transform * (col, row) gives (x, y) georeferenced coordinates\n",
        "                                contour_points_geo = [transform * (p[0], p[1]) for p in contour_points_padded]\n",
        "                                polygon = Polygon(contour_points_geo)\n",
        "                            else:\n",
        "                                # If no transform, use padded pixel coordinates for the polygon\n",
        "                                polygon = Polygon(contour_points_padded)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                # Add the polygon and its properties to the vector file\n",
        "                            if not polygon.is_empty and polygon.exterior: # Ensure polygon is valid and not empty\n",
        "                                # Cast score to a standard float type to potentially avoid Fiona warning\n",
        "                                collection.write({\n",
        "                                    'geometry': mapping(polygon),\n",
        "                                    'properties': {'score': float(score), 'class_id': int(class_id)},\n",
        "                                })\n",
        "\n",
        "print(f\"Vector file (Geopackage) saved to {OUTPUT_VECTOR_PATH}\")"
      ],
      "metadata": {
        "id": "9iOIBvZq9XBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bbf1d1c-8917-4e57-f888-ba62b9912899"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector file (Geopackage) saved to /content/drive/MyDrive/colabDL/Turkiye/TR1/Rubble_1.gpkg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Produce a .png image mosaic with the predictions and bounding boxes printed on the image."
      ],
      "metadata": {
        "id": "SCqAzQ2FhVTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import rasterio\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "import detectron2.structures\n",
        "import torch # Import torch for tensor operations if masks are included\n",
        "import math # Import math for ceil\n"
      ],
      "metadata": {
        "id": "KLiBdFc0gJvh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== USER PATHS ====\n",
        "INPUT_TIFF = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/1_GEOTIFF.tif\" # Use the georeferenced TIFF\n",
        "OUT_DIR   = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/Rubble_1.png\"  # Output directory for predicted tile PNGs\n",
        "TILE = 1024\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Read the georeferenced image and get georeferencing info\n",
        "try:\n",
        "    with rasterio.open(INPUT_TIFF) as src:\n",
        "        img = src.read()\n",
        "        # Rasterio reads bands first (C, H, W), convert to OpenCV format (H, W, C)\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "        # Explicitly select the first 3 channels to handle potential alpha channels\n",
        "        if img.shape[2] == 4:\n",
        "            img = img[:, :, :3]\n",
        "        orig_h, orig_w = img.shape[:2]\n",
        "        transform = src.transform # Affine transform for georeferencing\n",
        "        crs = src.crs # Coordinate reference system\n",
        "\n",
        "except rasterio.errors.RasterioIOError as e:\n",
        "    raise FileNotFoundError(f\"Could not read georeferenced image at {INPUT_TIFF}: {e}\")\n",
        "\n",
        "# Pad to multiples of TILE so we don’t lose right/bottom edges\n",
        "H = math.ceil(orig_h / TILE) * TILE\n",
        "W = math.ceil(orig_w / TILE) * TILE\n",
        "pad_bottom = H - orig_h\n",
        "pad_right  = W - orig_w\n",
        "\n",
        "if pad_bottom > 0 or pad_right > 0:\n",
        "    # Use the last pixel value for padding to avoid edge effects if possible,\n",
        "    # otherwise use black padding for color images.\n",
        "    if img.shape[2] == 3:\n",
        "        pad_val = [int(img[-1,-1,0]), int(img[-1,-1,1]), int(img[-1,-1,2])]\n",
        "    else:\n",
        "        pad_val = [0] * img.shape[2]\n",
        "    img_padded = cv2.copyMakeBorder(\n",
        "        img, 0, pad_bottom, 0, pad_right,\n",
        "        borderType=cv2.BORDER_CONSTANT, value=pad_val\n",
        "    )\n",
        "else:\n",
        "    img_padded = img\n",
        "\n",
        "rows = H // TILE\n",
        "cols = W // TILE\n",
        "\n",
        "tiles_written = 0\n",
        "\n",
        "# Use cfg_pretrained to get the dataset metadata for visualization\n",
        "# Check if DATASETS.TRAIN is defined and not empty\n",
        "if 'cfg_pretrained' in globals() and hasattr(cfg_pretrained.DATASETS, 'TRAIN') and cfg_pretrained.DATASETS.TRAIN:\n",
        "    metadata = MetadataCatalog.get(cfg_pretrained.DATASETS.TRAIN[0])\n",
        "else:\n",
        "    metadata = None # Use default metadata if not available\n",
        "\n",
        "\n",
        "for y in range(rows):\n",
        "    for x in range(cols):\n",
        "        r0 = y * TILE\n",
        "        c0 = x * TILE\n",
        "        tile = img_padded[r0:r0+TILE, c0:c0+TILE]\n",
        "\n",
        "        # Make prediction on the tile\n",
        "        outputs = predictor(tile)\n",
        "\n",
        "        # Visualize the predictions on the tile\n",
        "        # Convert tile to BGR for Visualizer\n",
        "        v = Visualizer(tile[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "        predicted_tile = out.get_image()[:, :, ::-1] # Convert back to RGB for saving as PNG\n",
        "\n",
        "        # Save the predicted tile\n",
        "        cv2.imwrite(os.path.join(OUT_DIR, f\"tile_{y}_{x}_pred.png\"), predicted_tile)\n",
        "        tiles_written += 1\n",
        "\n",
        "print(f\"Done. Wrote {tiles_written} predicted tiles to {OUT_DIR}\")"
      ],
      "metadata": {
        "id": "6jyU-88pI_ye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bba59525-684f-483d-8fa9-9113e29e2843"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. Wrote 80 predicted tiles to /content/drive/MyDrive/colabDL/Turkiye/TR1/Rubble_1.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69ed2b7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3799c790-15cf-4970-cfcb-2538c0b3e0d6"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Directory containing the predicted tiles\n",
        "PREDICTED_TILES_DIR = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/Rubble_1.png\"\n",
        "# Output path for the remosaiced image\n",
        "REMOsaICED_OUTPUT_PATH = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/Rubble_mosaic_1.png\"\n",
        "TILE_SIZE = 1024 # The size of each tile (should match the TILE size used for tiling)\n",
        "\n",
        "# Get all the predicted tile filenames\n",
        "tile_filenames = [f for f in os.listdir(PREDICTED_TILES_DIR) if f.endswith('_pred.png')]\n",
        "\n",
        "if not tile_filenames:\n",
        "    print(\"No predicted tile images found in the directory.\")\n",
        "else:\n",
        "    # Extract rows and columns from filenames to determine the grid size\n",
        "    max_row = 0\n",
        "    max_col = 0\n",
        "    tile_info = {}\n",
        "    for filename in tile_filenames:\n",
        "        match = re.match(r'tile_(\\d+)_(\\d+)_pred\\.png', filename)\n",
        "        if match:\n",
        "            row = int(match.group(1))\n",
        "            col = int(match.group(2))\n",
        "            tile_info[(row, col)] = filename\n",
        "            max_row = max(max_row, row)\n",
        "            max_col = max(max_col, col)\n",
        "\n",
        "    rows = max_row + 1\n",
        "    cols = max_col + 1\n",
        "\n",
        "    # Determine the dimensions of the remosaiced image\n",
        "    # Read one tile to get image dimensions (assuming all tiles have the same dimensions)\n",
        "    sample_tile_path = os.path.join(PREDICTED_TILES_DIR, tile_filenames[0])\n",
        "    sample_tile = cv2.imread(sample_tile_path)\n",
        "    tile_h, tile_w, tile_c = sample_tile.shape\n",
        "\n",
        "    # Calculate the full image dimensions, considering potential padding\n",
        "    # The padded dimensions should match the H and W calculated during tiling\n",
        "    # However, if we saved the predicted tiles without removing the padding visualization,\n",
        "    # the size will be a multiple of TILE_SIZE.\n",
        "    full_h = rows * TILE_SIZE\n",
        "    full_w = cols * TILE_SIZE\n",
        "\n",
        "    # Create a blank canvas for the remosaiced image\n",
        "    remosaiced_img = np.zeros((full_h, full_w, tile_c), dtype=np.uint8)\n",
        "\n",
        "    # Place each tile onto the canvas\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            if (r, c) in tile_info:\n",
        "                tile_filename = tile_info[(r, c)]\n",
        "                tile_path = os.path.join(PREDICTED_TILES_DIR, tile_filename)\n",
        "                tile_img = cv2.imread(tile_path)\n",
        "\n",
        "                # Ensure the tile image size matches the expected TILE_SIZE\n",
        "                if tile_img.shape[:2] == (TILE_SIZE, TILE_SIZE):\n",
        "                     remosaiced_img[r * TILE_SIZE:(r + 1) * TILE_SIZE, c * TILE_SIZE:(c + 1) * TILE_SIZE] = tile_img\n",
        "                else:\n",
        "                     print(f\"Warning: Tile {tile_filename} has unexpected dimensions {tile_img.shape[:2]}. Expected ({TILE_SIZE}, {TILE_SIZE}). Skipping.\")\n",
        "\n",
        "\n",
        "    # Save the remosaiced image\n",
        "    cv2.imwrite(REMOsaICED_OUTPUT_PATH, remosaiced_img)\n",
        "\n",
        "    print(f\"Remosaiced image saved to {REMOsaICED_OUTPUT_PATH}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remosaiced image saved to /content/drive/MyDrive/colabDL/Turkiye/TR1/Rubble_mosaic_1.png\n"
          ]
        }
      ]
    }
  ]
}