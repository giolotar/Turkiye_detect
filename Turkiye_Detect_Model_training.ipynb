{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN+KF99Q5tGT2fn+66Yso4F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giolotar/Turkiye_detect/blob/main/Turkiye_Detect_Model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rapid response Mapping of Rubble Detection in the cities of Antakya and Kahramanmaraş, Turkiye**\n",
        "\n",
        "### The following Project utilizes Mask RCNN for Rubble detection following the 7.8 Magnitude earthquake that struck the Gazantiep Region of Turkey.\n",
        "\n",
        "The cities of Antakya and Kharamanmaras have been majorly hit by the earthquake.\n",
        "\n",
        "The project utilises the detectron2 library to construct the model and run inference. Two very high resolution satellite imagery have been selected for the city of Antakya and Kharamanmaras.\n",
        "\n",
        "The image from the city of Antakya has been selected to conduct training and validation of the model, while the city of Kharamanmaras to run testing and model generalization.\n",
        "\n",
        "The image has been tiled into 1024x1024 patches, COCO-format annotations have been created on CVAT. This is a relatively small dataset aiming to run rapid inference on the status of rubble\n",
        "\n",
        "The products are a shapefile which can be utilized to generate a webMap or for further analysis by emergency response Agencies"
      ],
      "metadata": {
        "id": "KlWAZdevvcKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SET-UP**\n",
        "\n",
        "The set up includes installing the dependencies and importing the required Detectron2 Libraries."
      ],
      "metadata": {
        "id": "rOP3uJkjybpx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44CaW_qRvXB2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Detectron2 Library\n",
        "!python -m pip install pyyaml==5.1\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "#Install geospatial libraries\n",
        "!pip install rasterio fiona shapely"
      ],
      "metadata": {
        "id": "4zD6LdLJBZ-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ],
      "metadata": {
        "id": "fpp663Az5KAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ7HdUtkPWaI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os, json, cv2, random, math, re\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "#import geospatial libraries\n",
        "from shapely.geometry import Polygon, mapping\n",
        "import fiona\n",
        "import rasterio\n",
        "from rasterio.enums import ColorInterp\n",
        "import glob\n",
        "\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Dataset Creation**\n",
        "\n",
        "To run inference for instance segmentation, the model requires COCO-annotations in a JSON format.\n",
        "\n",
        "There are multiple online softwares for making annotations; for this exercise CVAT has been used https://app.cvat.ai/.\n",
        "\n",
        "Detectron2 makes it very easy to register instances with few lines of code."
      ],
      "metadata": {
        "id": "yL_lI3fq5fIo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjLthmq44JZA"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Here we register the coco-annotations (instances) for both the train dataset and validation dataset providing the path to the json files and the folders with the respective images\n",
        "register_coco_instances(\"dataset_train\", {}, \"/content/drive/MyDrive/colabDL/tile1/train_ann/train_ann.json\", \"/content/drive/MyDrive/colabDL/tile1/train_row6_7\")\n",
        "register_coco_instances(\"dataset_val\", {}, \"/content/drive/MyDrive/colabDL/tile1/val_ann/val_ann.json\", \"/content/drive/MyDrive/colabDL/tile1/Val_row9\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZdMUqrwQJcc"
      },
      "outputs": [],
      "source": [
        "train_metadata = MetadataCatalog.get(\"dataset_train\")\n",
        "train_dataset_dicts = DatasetCatalog.get(\"dataset_train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9zFxqwUQLtI"
      },
      "outputs": [],
      "source": [
        "val_metadata = MetadataCatalog.get(\"dataset_val\")\n",
        "val_dataset_dicts = DatasetCatalog.get(\"dataset_val\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's Visualize some random Samples from our Validation and Train Dataset"
      ],
      "metadata": {
        "id": "pTj0yDzl64Ej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpfY3gRyQS63"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ86bxTMQXry"
      },
      "outputs": [],
      "source": [
        "# Visualize some random samples from the train dataset\n",
        "for d in random.sample(val_dataset_dicts, 2):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)\n",
        "    vis = visualizer.draw_dataset_dict(d)\n",
        "    plt.imshow(vis.get_image()[:, :, ::-1])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TP-2lnfwQZgj"
      },
      "outputs": [],
      "source": [
        "# Visualize some random samples from the train dataset\n",
        "for d in random.sample(train_dataset_dicts, 2):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=train_metadata, scale=0.5)\n",
        "    vis = visualizer.draw_dataset_dict(d)\n",
        "    plt.imshow(vis.get_image()[:, :, ::-1])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING**\n",
        "\n",
        "In the following lines of code we can begin training our Model.\n",
        "\n",
        "First we have to create a configuration file to build the Hyperparameters of our MRCNN Model"
      ],
      "metadata": {
        "id": "uxwAyESX7UqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.modeling import build_model"
      ],
      "metadata": {
        "id": "mb9k92ah7UY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Begin by creating a configuration variable cfg\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.OUTPUT_DIR = \"/content\" #path where the configuration file .yaml will be saved\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")) #this is a preconfigured Mask RCNN model from the Model Zoo of Detectron2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\") #pre-trained weights to kickstart model training.\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"dataset_train\",) #specifies the training dataset to use\n",
        "cfg.DATASETS.TEST = () #this is to add a test dataset. ! in this case no test dataset is being used for it is a rapid model inference building\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "\n",
        "cfg.SOLVER.IMS_PER_BATCH = 4  # refers to the number of images included in each batch that the model's optimizer processes during one training step\n",
        "cfg.SOLVER.BASE_LR = 0.005  # How to pick a good learning rate? Trick = BASE_LR = 0.02 × (IMS_PER_BATCH / 16)\n",
        "cfg.SOLVER.SCHEDULER_NAME = \"WarmupMultiStepLR\" #cfg.SOLVER.SCHEDULER_NAME = \"WarmupCosineLR\"\n",
        "\n",
        "#Scheduling Iterations\n",
        "cfg.SOLVER.MAX_ITER = 1000 #number of iterations of the model. To adjust based on computing power and dataset size\n",
        "cfg.SOLVER.STEPS = (600, 800)     # decay at 60% and 80% of all the iterations\n",
        "cfg.SOLVER.GAMMA = 0.1\n",
        "\n",
        "# WARMUP iterations allow for intial model training\n",
        "cfg.SOLVER.WARMUP_METHOD = \"linear\"\n",
        "cfg.SOLVER.WARMUP_ITERS = 500\n",
        "cfg.SOLVER.WARMUP_FACTOR = 1.0 / 10000\n",
        "\n",
        "#anchors for small buildings\n",
        "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[16], [32], [64], [128], [256]] #Sets the anchor box scales\n",
        "cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.5, 0.75, 1.0, 1.33, 2.0]] #width:height ratios for anchors at every FPN level.\n",
        "cfg.MODEL.RPN.PRE_NMS_TOPK_TRAIN = 4000\n",
        "cfg.MODEL.RPN.PRE_NMS_TOPK_TEST  = 4000\n",
        "cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN = 2000\n",
        "cfg.MODEL.RPN.POST_NMS_TOPK_TEST  = 2000\n",
        "cfg.MODEL.RPN.NMS_THRESH = 0.6\n",
        "\n",
        "# Correcting IN_FEATURES to match the backbone output before FPN\n",
        "cfg.MODEL.FPN.IN_FEATURES = [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
        "\n",
        "#this resizes the tiles even further; configuration set for tiles that are 1024x1024\n",
        "cfg.INPUT.MIN_SIZE_TRAIN = (1024, 1280, 1400)\n",
        "cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING = \"choice\"\n",
        "cfg.INPUT.MAX_SIZE_TRAIN = 1800\n",
        "cfg.INPUT.MIN_SIZE_TEST = 1400\n",
        "cfg.INPUT.MAX_SIZE_TEST = 1800\n",
        "\n",
        "#ROI Sampling\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 1024\n",
        "cfg.MODEL.ROI_HEADS.POSITIVE_FRACTION = 0.5\n",
        "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.4\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
        "cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG = True\n",
        "\n",
        "\n",
        "# Quality-of-life\n",
        "cfg.SOLVER.AMP.ENABLED = True\n",
        "cfg.TEST.EVAL_PERIOD = 100           # evaluates every 1000 iters\n",
        "cfg.SOLVER.CHECKPOINT_PERIOD = 100   # Saves the model every 1000 iterations\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) #Create an instance of of DefaultTrainer with the given congiguration\n",
        "trainer.resume_or_load(resume=False) #Load a pretrained model if available (resume training) or start training from scratch if no pretrained model is available"
      ],
      "metadata": {
        "id": "_BiOqoo5eo2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train() #Start the training process"
      ],
      "metadata": {
        "id": "RBmvduFHCtnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "# Save the configuration to a config.yaml file\n",
        "config_yaml_path = \"\"\n",
        "with open(config_yaml_path, 'w') as file:\n",
        "    yaml.dump(cfg, file)"
      ],
      "metadata": {
        "id": "M6DyqY6rC1tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference on Test Images**\n",
        "\n",
        "We run inference by generating a prediction setting the testing thershold"
      ],
      "metadata": {
        "id": "gKVwGPpzDOG8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNVq6vHxcs3"
      },
      "outputs": [],
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"/content/model_finale.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg) #predictor draws from the yaml configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAyh5vaNx1xE"
      },
      "source": [
        "## EVALUATION **Check** average precision and recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-dvhXP0xv6T"
      },
      "outputs": [],
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "\n",
        "evaluator = COCOEvaluator(\"dataset_val\", output_dir=\"./output\")\n",
        "val_loader = build_detection_test_loader(cfg, \"dataset_val\")\n",
        "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "m,ore evaluation metrics for the model"
      ],
      "metadata": {
        "id": "6oASpXzQAxWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference on Out of Domain Images**\n",
        "\n",
        "Then we run the predictor on an unseen Georeferenced TIF image, the image gets tiled, predictions are run on the tiles and exported in .npz as georeferenced polygons.\n",
        "Then they are turned into a shapefile"
      ],
      "metadata": {
        "id": "bex9MRnxWrwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Shapefile with predicted instances as Georeferenced Polygons"
      ],
      "metadata": {
        "id": "EygAnXpPW1e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== USER PATHS ====\n",
        "INPUT_TIFF = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/1_GEOTIFF.tif\" # Use the georeferenced TIFF\n",
        "OUT_DIR   = \"/content/drive/MyDrive/colabDL/Turkiye/TR1\"  # main output directory\n",
        "NPZ_OUT_DIR = os.path.join(OUT_DIR, \"npz_preds\") # subdirectory for npz files\n",
        "TILE = 1024\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(NPZ_OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Read the georeferenced image and get georeferencing info\n",
        "try:\n",
        "    with rasterio.open(INPUT_TIFF) as src:\n",
        "        img = src.read()\n",
        "        # Rasterio reads bands first (C, H, W), convert to OpenCV format (H, W, C)\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "        # Explicitly select the first 3 channels to handle potential alpha channels\n",
        "        if img.shape[2] == 4:\n",
        "            img = img[:, :, :3]\n",
        "        orig_h, orig_w = img.shape[:2]\n",
        "        transform = src.transform # Affine transform for georeferencing\n",
        "        crs = src.crs # Coordinate reference system\n",
        "\n",
        "except rasterio.errors.RasterioIOError as e:\n",
        "    raise FileNotFoundError(f\"Could not read georeferenced image at {INPUT_TIFF}: {e}\")\n",
        "\n",
        "# Pad to multiples of TILE so we don’t lose right/bottom edges\n",
        "H = math.ceil(orig_h / TILE) * TILE\n",
        "W = math.ceil(orig_w / TILE) * TILE\n",
        "pad_bottom = H - orig_h\n",
        "pad_right  = W - orig_w\n",
        "\n",
        "if pad_bottom or pad_right:\n",
        "    # Use the last pixel value for padding to avoid edge effects if possible,\n",
        "    # otherwise use black padding for color images.\n",
        "    if img.shape[2] == 3:\n",
        "        pad_val = [int(img[-1,-1,0]), int(img[-1,-1,1]), int(img[-1,-1,2])]\n",
        "    else:\n",
        "        pad_val = [0] * img.shape[2]\n",
        "    img = cv2.copyMakeBorder(\n",
        "        img, 0, pad_bottom, 0, pad_right,\n",
        "        borderType=cv2.BORDER_CONSTANT, value=pad_val\n",
        "    )\n",
        "\n",
        "rows = H // TILE\n",
        "cols = W // TILE\n",
        "\n",
        "tiles_written = 0\n",
        "dets_saved = 0\n",
        "\n",
        "for y in range(rows):\n",
        "    for x in range(cols):\n",
        "        r0 = y * TILE\n",
        "        c0 = x * TILE\n",
        "        tile = img[r0:r0+TILE, c0:c0+TILE]\n",
        "\n",
        "        # Calculate georeferenced origin of the tile (top-left corner)\n",
        "        geo_x0, geo_y0 = transform * (c0, r0)\n",
        "\n",
        "        # ---- Run inference on the tile\n",
        "        outputs = predictor(tile)\n",
        "        inst = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "        # ---- Extract raw predictions\n",
        "        boxes  = inst.pred_boxes.tensor.numpy() if inst.has(\"pred_boxes\") else np.zeros((0,4), dtype=np.float32)\n",
        "        scores = inst.scores.numpy()             if inst.has(\"scores\")     else np.zeros((0,), dtype=np.float32)\n",
        "        classes= inst.pred_classes.numpy()       if inst.has(\"pred_classes\") else np.zeros((0,), dtype=np.int32)\n",
        "        masks  = inst.pred_masks.numpy()         if inst.has(\"pred_masks\") else None  # (N, H, W) boolean\n",
        "\n",
        "        # ---- SAVE RAW PREDICTIONS (compressed)\n",
        "        # Include tile origin in pixel and georeferenced coordinates\n",
        "        npz_path = os.path.join(NPZ_OUT_DIR, f\"tile_{y}_{x}_pred.npz\")\n",
        "        np.savez_compressed(\n",
        "            npz_path,\n",
        "            boxes=boxes,\n",
        "            scores=scores,\n",
        "            classes=classes,\n",
        "            masks=masks,                 # None is allowed; np.load(..., allow_pickle=True) will handle it\n",
        "            tile_row=y,\n",
        "            tile_col=x,\n",
        "            tile_origin_rc=np.array([r0, c0], dtype=np.int32), # Pixel origin (row, col)\n",
        "            tile_origin_geo=np.array([geo_x0, geo_y0], dtype=np.float64), # Georeferenced origin (x, y)\n",
        "            tile_size=TILE,\n",
        "            padded_h=H,\n",
        "            padded_w=W,\n",
        "            orig_h=orig_h,\n",
        "            orig_w=orig_w,\n",
        "            pad_bottom=pad_bottom,\n",
        "            pad_right=pad_right\n",
        "        )\n",
        "        dets_saved += len(boxes)\n",
        "\n",
        "print(f\"Done. Wrote {tiles_written} visualized tiles to {OUT_DIR}\")\n",
        "print(f\"Saved predictions for {rows*cols} tiles ({dets_saved} detections total) as .npz files to {NPZ_OUT_DIR}\")"
      ],
      "metadata": {
        "id": "GOscjzoCAxr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert from the npz to shp retaining geographic coordinates"
      ],
      "metadata": {
        "id": "X2UAL8b3Axr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing the predicted tile .npz files\n",
        "PREDICTED_NPZ_DIR = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/npz_preds\" # Assuming npz files are in a subdirectory\n",
        "\n",
        "OUTPUT_VECTOR_PATH = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/Rubble_1.gpkg\"\n",
        "\n",
        "ORIGINAL_GEOREFERENCED_IMAGE_PATH = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/1_GEOTIFF.tif\" # Example path, replace with your actual georeferenced image path\n",
        "\n",
        "# --- Function to get georeferencing information ---\n",
        "def get_georeferencing_info(image_path):\n",
        "    \"\"\"Reads georeferencing information (transform and CRS) from a georeferenced image.\"\"\"\n",
        "    try:\n",
        "        with rasterio.open(image_path) as src:\n",
        "            return src.transform, src.crs\n",
        "    except rasterio.errors.RasterioIOError as e:\n",
        "        print(f\"Error reading georeferencing info from {image_path}: {e}\")\n",
        "        print(\"Assuming pixel coordinates. The output vector file will NOT be georeferenced.\")\n",
        "        return None, None\n",
        "\n",
        "# Get georeferencing info from the original image\n",
        "transform, crs = get_georeferencing_info(ORIGINAL_GEOREFERENCED_IMAGE_PATH)\n",
        "\n",
        "# --- Define the schema for the vector file ---\n",
        "\n",
        "schema = {\n",
        "    'geometry': 'Polygon',\n",
        "    'properties': {'score': 'float', 'class_id': 'int'},\n",
        "}\n",
        "\n",
        "# --- Create the vector file (Geopackage) ---\n",
        "driver = 'GPKG'\n",
        "\n",
        "with fiona.open(\n",
        "    OUTPUT_VECTOR_PATH,\n",
        "    'w',\n",
        "    driver,\n",
        "    schema,\n",
        "    crs=crs # Use the obtained CRS, or None if not available\n",
        ") as collection:\n",
        "    # List all .npz files in the directory\n",
        "    npz_filenames = [f for f in os.listdir(PREDICTED_NPZ_DIR) if f.endswith('_pred.npz')]\n",
        "\n",
        "    if not npz_filenames:\n",
        "        print(\"No predicted .npz files found in the directory.\")\n",
        "    else:\n",
        "        # Iterate through each .npz file\n",
        "        for filename in npz_filenames:\n",
        "            npz_path = os.path.join(PREDICTED_NPZ_DIR, filename)\n",
        "            with np.load(npz_path, allow_pickle=True) as data:\n",
        "                masks = data['masks']\n",
        "                scores = data['scores']\n",
        "                classes = data['classes']\n",
        "                tile_origin_rc = data['tile_origin_rc'] # Pixel origin (r0, c0)\n",
        "\n",
        "\n",
        "                if masks is not None and masks.shape[0] > 0:\n",
        "                    # Process each instance mask within the tile\n",
        "                    for i in range(masks.shape[0]):\n",
        "                        instance_mask = masks[i]\n",
        "                        score = scores[i]\n",
        "                        class_id = classes[i]\n",
        "\n",
        "\n",
        "                        mask_uint8 = instance_mask.astype(np.uint8) * 255\n",
        "                        # Use cv2.RETR_TREE and cv2.CHAIN_APPROX_NONE for more detailed contours if needed,\n",
        "                        # but SIMPLE is usually sufficient for polygons.\n",
        "                        contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "                        # Convert contours to polygons and transform coordinates\n",
        "                        for contour in contours:\n",
        "\n",
        "                            contour_points_tile = contour.squeeze().tolist()\n",
        "\n",
        "                            if not isinstance(contour_points_tile, list):\n",
        "                                # Handle cases where contour.squeeze() results in a single point\n",
        "                                contour_points_tile = [contour_points_tile]\n",
        "                            elif not all(isinstance(p, list) for p in contour_points_tile):\n",
        "                                # Handle cases where contour.squeeze() results in a flat list of coords\n",
        "                                contour_points_tile = [contour_points_tile]\n",
        "\n",
        "\n",
        "                            if len(contour_points_tile) < 3:\n",
        "                                continue # Skip if not enough points to form a polygon\n",
        "\n",
        "                            # Convert tile pixel coordinates to padded image pixel coordinates\n",
        "                            # Tile origin is (row, col), contour points are (col, row)\n",
        "                            contour_points_padded = [(p[0] + tile_origin_rc[1], p[1] + tile_origin_rc[0]) for p in contour_points_tile]\n",
        "\n",
        "                            # Convert padded image pixel coordinates to geographic coordinates using the original image's transform\n",
        "                            if transform:\n",
        "                                # Apply the affine transform to each point\n",
        "                                # transform * (col, row) gives (x, y) georeferenced coordinates\n",
        "                                contour_points_geo = [transform * (p[0], p[1]) for p in contour_points_padded]\n",
        "                                polygon = Polygon(contour_points_geo)\n",
        "                            else:\n",
        "                                # If no transform, use padded pixel coordinates for the polygon\n",
        "                                polygon = Polygon(contour_points_padded)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                # Add the polygon and its properties to the vector file\n",
        "                            if not polygon.is_empty and polygon.exterior: # Ensure polygon is valid and not empty\n",
        "                                # Cast score to a standard float type to potentially avoid Fiona warning\n",
        "                                collection.write({\n",
        "                                    'geometry': mapping(polygon),\n",
        "                                    'properties': {'score': float(score), 'class_id': int(class_id)},\n",
        "                                })\n",
        "\n",
        "print(f\"Vector file (Geopackage) saved to {OUTPUT_VECTOR_PATH}\")"
      ],
      "metadata": {
        "id": "DmWj0GCqAxr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Produce a .TIF image mosaic with the predictions and bounding boxes printed on the image."
      ],
      "metadata": {
        "id": "SCqAzQ2FhVTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use npz to create a geotiff\n",
        "\n",
        "# ==== USER PATHS ====\n",
        "INPUT_TIFF = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/1_GEOTIFF.tif\"  # original georeferenced image\n",
        "OUT_DIR    = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/Rubble_2_tif\"   # output directory\n",
        "OUTPUT_GEOTIFF = os.path.join(OUT_DIR, \"Rubble_1_predictions_vis.tif\")   # final mosaic GeoTIFF\n",
        "PREDICTED_NPZ_DIR = \"/content/drive/MyDrive/colabDL/Turkiye/TR1/npz_preds\"  # folder with *_pred.npz\n",
        "TILE = 1024\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def color_from_class_id(class_id: int) -> tuple:\n",
        "\n",
        "    rng = np.random.RandomState(class_id * 1337 + 42)\n",
        "    return tuple(int(x) for x in rng.choice(256, size=3))  # BGR\n",
        "\n",
        "def put_label(img, text, pt, bg=(0,0,0), fg=(255,255,255)):\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    scale = 0.5\n",
        "    thickness = 1\n",
        "    (tw, th), baseline = cv2.getTextSize(text, font, scale, thickness)\n",
        "    x, y = pt\n",
        "    cv2.rectangle(img, (x, y - th - baseline), (x + tw, y + baseline), bg, -1, cv2.LINE_AA)\n",
        "    cv2.putText(img, text, (x, y), font, scale, fg, thickness, cv2.LINE_AA)\n",
        "\n",
        "# ---------- visualization config (ADD THIS) ----------\n",
        "USE_FIXED_COLOR = True                # force one color for all instances\n",
        "FIXED_COLOR_BGR = (0, 0, 255)         # red in BGR\n",
        "ALPHA = 0.45                          # mask fill opacity\n",
        "DRAW_BBOX = True                      # draw bounding boxes\n",
        "LABEL_OFFSET = (2, 12)                # (x,y) offset inside top-left corner of bbox for the label\n",
        "\n",
        "# ---------- read base imagery & georef ----------\n",
        "try:\n",
        "    with rasterio.open(INPUT_TIFF) as src:\n",
        "        base = src.read()                # (C,H,W)\n",
        "        transform = src.transform\n",
        "        crs = src.crs\n",
        "except rasterio.errors.RasterioIOError as e:\n",
        "    raise FileNotFoundError(f\"Could not read georeferenced image at {INPUT_TIFF}: {e}\")\n",
        "\n",
        "# Convert to HWC (OpenCV) and force 3 channels for visualization\n",
        "base = np.transpose(base, (1, 2, 0))     # (H,W,C)\n",
        "if base.shape[2] >= 3:\n",
        "    base_rgb = base[:, :, :3]\n",
        "else:\n",
        "    base_rgb = np.repeat(base, 3, axis=2)\n",
        "\n",
        "orig_h, orig_w = base_rgb.shape[:2]\n",
        "\n",
        "# ---------- pad canvas to tile grid (optional but handy) ----------\n",
        "H = math.ceil(orig_h / TILE) * TILE\n",
        "W = math.ceil(orig_w / TILE) * TILE\n",
        "pad_bottom = H - orig_h\n",
        "pad_right  = W - orig_w\n",
        "\n",
        "if pad_bottom > 0 or pad_right > 0:\n",
        "    pad_val = [int(base_rgb[-1,-1,0]), int(base_rgb[-1,-1,1]), int(base_rgb[-1,-1,2])]\n",
        "    canvas = cv2.copyMakeBorder(base_rgb, 0, pad_bottom, 0, pad_right,\n",
        "                                borderType=cv2.BORDER_CONSTANT, value=pad_val)\n",
        "else:\n",
        "    canvas = base_rgb.copy()\n",
        "\n",
        "# We will draw directly onto canvas (BGR for OpenCV convenience)\n",
        "canvas = canvas[:, :, ::-1]  # convert to BGR\n",
        "\n",
        "# ---------- iterate over npz tiles and draw ----------\n",
        "npz_files = sorted([f for f in os.listdir(PREDICTED_NPZ_DIR) if f.endswith(\"_pred.npz\")])\n",
        "if not npz_files:\n",
        "    print(f\"No *_pred.npz files found in {PREDICTED_NPZ_DIR}\")\n",
        "\n",
        "instances_drawn = 0\n",
        "\n",
        "for fname in npz_files:\n",
        "    fpath = os.path.join(PREDICTED_NPZ_DIR, fname)\n",
        "    try:\n",
        "        with np.load(fpath, allow_pickle=True) as data:\n",
        "            masks = data.get('masks', None)        # (N, th, tw) boolean/0-1\n",
        "            scores = data.get('scores', None)      # (N,)\n",
        "            classes = data.get('classes', None)    # (N,)\n",
        "            tile_origin_rc = data.get('tile_origin_rc', None)  # (r0, c0)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {fname}: cannot read npz ({e})\")\n",
        "        continue\n",
        "\n",
        "    if masks is None or masks.size == 0:\n",
        "        continue\n",
        "    if tile_origin_rc is None:\n",
        "\n",
        "        try:\n",
        "            base_name = os.path.splitext(fname)[0]\n",
        "            parts = base_name.split('_')\n",
        "            r0 = int([p for p in parts if p.startswith('r')][0][1:])\n",
        "            c0 = int([p for p in parts if p.startswith('c')][0][1:])\n",
        "        except Exception:\n",
        "            print(f\"Skipping {fname}: missing tile_origin_rc and filename not parseable.\")\n",
        "            continue\n",
        "    else:\n",
        "        r0, c0 = int(tile_origin_rc[0]), int(tile_origin_rc[1])\n",
        "\n",
        "    N, th, tw = masks.shape\n",
        "    # Bounds on canvas\n",
        "    r1 = r0 + th\n",
        "    c1 = c0 + tw\n",
        "    if r0 >= canvas.shape[0] or c0 >= canvas.shape[1]:\n",
        "        continue  # tile completely outside padded canvas\n",
        "    # Clip to canvas in case of edge/padding mismatch\n",
        "    rr0, cc0 = max(0, r0), max(0, c0)\n",
        "    rr1, cc1 = min(canvas.shape[0], r1), min(canvas.shape[1], c1)\n",
        "    if rr1 <= rr0 or cc1 <= cc0:\n",
        "        continue\n",
        "\n",
        "    # Region of interest on canvas (make a contiguous copy for drawing)\n",
        "    roi = canvas[rr0:rr1, cc0:cc1].copy()  # Make a copy here\n",
        "\n",
        "    # Matching region in tile coordinates\n",
        "    tr0 = rr0 - r0\n",
        "    tc0 = cc0 - c0\n",
        "    tr1 = th - (r1 - rr1)\n",
        "    tc1 = tw - (c1 - cc1)\n",
        "\n",
        "    # Draw each instance\n",
        "    for i in range(N):\n",
        "        m = masks[i, tr0:tr1, tc0:tc1].astype(np.uint8)  # (h,w) 0/1\n",
        "        if m.max() == 0:\n",
        "            continue\n",
        "        score = float(scores[i]) if scores is not None else None\n",
        "        class_id = int(classes[i]) if classes is not None else 0\n",
        "\n",
        "        bgr = FIXED_COLOR_BGR if USE_FIXED_COLOR else color_from_class_id(class_id)\n",
        "\n",
        "        alpha = ALPHA\n",
        "\n",
        "        # Overlay fill (mask tint)\n",
        "        colored = np.zeros_like(roi, dtype=np.uint8)\n",
        "        colored[:] = bgr\n",
        "        mask3 = np.repeat(m[:, :, None], 3, axis=2)  # (h,w,3)\n",
        "        roi[:] = np.where(mask3 == 1, (alpha * colored + (1 - alpha) * roi).astype(np.uint8), roi)\n",
        "\n",
        "        # Contour outline (optional; keeping it helps visually)\n",
        "        contours, _ = cv2.findContours((m * 255), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        cv2.drawContours(roi, contours, -1, bgr, 2, cv2.LINE_AA)\n",
        "\n",
        "        # ---- draw bounding box from the mask ----\n",
        "        if DRAW_BBOX:\n",
        "            ys, xs = np.where(m > 0)\n",
        "            if xs.size and ys.size:\n",
        "                x0, y0 = int(xs.min()), int(ys.min())\n",
        "                x1, y1 = int(xs.max()), int(ys.max())\n",
        "                cv2.rectangle(roi, (x0, y0), (x1, y1), bgr, 2, cv2.LINE_AA)\n",
        "\n",
        "        # ---- label at top-left of bbox ----\n",
        "        if score is not None:\n",
        "            pct = int(round(score * 100)) if 0.0 <= score <= 1.0 else int(round(score))\n",
        "            label = f\"{pct}%\"\n",
        "        else:\n",
        "            label = \"\"\n",
        "\n",
        "        if label:\n",
        "            # If we have a bbox, anchor to its top-left; otherwise, fallback to contour’s top-left\n",
        "            if DRAW_BBOX and xs.size and ys.size:\n",
        "                x0_lbl, y0_lbl = x0, y0\n",
        "            else:\n",
        "                # Fallback: use contour’s top-left from the largest contour (by area)\n",
        "                if contours:\n",
        "                    areas = [cv2.contourArea(c) for c in contours]\n",
        "                    k = int(np.argmax(areas))\n",
        "                    x0_lbl, y0_lbl, w_lbl, h_lbl = cv2.boundingRect(contours[k])\n",
        "                else:\n",
        "                    x0_lbl, y0_lbl = 0, 0\n",
        "\n",
        "            # Apply an offset so text is *inside* the region\n",
        "            ox, oy = LABEL_OFFSET\n",
        "            lx = max(0, min(x0_lbl + ox, roi.shape[1] - 1))\n",
        "            ly = max(12, min(y0_lbl + oy, roi.shape[0] - 1))\n",
        "            put_label(roi, label, (lx, ly), bg=(0, 0, 0), fg=(255, 255, 255))\n",
        "\n",
        "        instances_drawn += 1\n",
        "\n",
        "    canvas[rr0:rr1, cc0:cc1] = roi\n",
        "\n",
        "\n",
        "# ---------- crop back and write geotiff ----------\n",
        "canvas = canvas[:, :, ::-1]  # back to RGB\n",
        "mosaic_vis_cropped = canvas[:orig_h, :orig_w, :]\n",
        "\n",
        "profile = {\n",
        "    \"driver\": \"GTiff\",\n",
        "    \"height\": orig_h,\n",
        "    \"width\":  orig_w,\n",
        "    \"count\":  3,\n",
        "    \"dtype\":  \"uint8\",\n",
        "    \"crs\":    crs,\n",
        "    \"transform\": transform,\n",
        "    \"compress\": \"lzw\",\n",
        "    \"photometric\": \"RGB\",\n",
        "    \"interleave\": \"pixel\",\n",
        "}\n",
        "\n",
        "with rasterio.open(OUTPUT_GEOTIFF, \"w\", **profile) as dst:\n",
        "    dst.write(np.transpose(mosaic_vis_cropped, (2, 0, 1)))\n",
        "    dst.colorinterp = (ColorInterp.red, ColorInterp.green, ColorInterp.blue)\n",
        "\n",
        "print(f\"Done. Drew {instances_drawn} instances from {len(npz_files)} tiles.\")\n",
        "print(f\"Wrote mosaic GeoTIFF with printed predictions to:\\n{OUTPUT_GEOTIFF}\")"
      ],
      "metadata": {
        "id": "I2FYQPwNmAU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the Image with the predicted instances"
      ],
      "metadata": {
        "id": "9WW4d-efx0II"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10592dd0"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "image_to_display_path = OUTPUT_GEOTIFF\n",
        "\n",
        "image_to_display = cv2.imread(image_to_display_path, -1)\n",
        "\n",
        "if image_to_display is None:\n",
        "    print(f\"Error: Could not read image at {image_to_display_path}. Please check the file path.\")\n",
        "else:\n",
        "\n",
        "    print(f\"Displaying image from: {image_to_display_path}\")\n",
        "    cv2_imshow(image_to_display)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}